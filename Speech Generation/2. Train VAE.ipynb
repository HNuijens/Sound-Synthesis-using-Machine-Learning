{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d77ed4c",
   "metadata": {},
   "source": [
    "<h1 style = \"font-size:3rem;color:darkcyan\"> Train VAE model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10124f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f32f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "def import_dataset(dataset_path):\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # split list into different np arrays\n",
    "    inputs = np.array(data['log_spectrogram'])\n",
    "    filenames = np.array(data['filenames'])\n",
    "    min_max_values = np.array(data['min_max_values'])\n",
    "    \n",
    "    # reshape to add one dimension to features for CNN\n",
    "    inputs = inputs[..., np.newaxis] \n",
    "    return inputs, filenames, min_max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b239200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, filenames, min_max_values = import_dataset('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "635419fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 256, 64, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape # [# inputs, # freq bins, # time frames, # 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd7c55",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e16cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, \\\n",
    "BatchNormalization, Flatten, Dense, Reshape, Conv2DTranspose, \\\n",
    "Activation, Lambda\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8901aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    \n",
    "    def __init__(self, \n",
    "                input_shape,\n",
    "                conv_filters,\n",
    "                conv_kernels,\n",
    "                conv_strides,\n",
    "                latent_space_dim):\n",
    "        \n",
    "        self.input_shape = input_shape \n",
    "        self.conv_filters = conv_filters\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        self.alpha = 1000000\n",
    "        \n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.model = None\n",
    "        \n",
    "        self._num_conv_layers = len(conv_filters)\n",
    "        self._shape_before_bottleneck = None\n",
    "        self._model_input = None\n",
    "        \n",
    "        self._build()\n",
    "        \n",
    "    def summary(self):\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "        self.model.summary()\n",
    "    \n",
    "    def compile(self, learning_rate = 0.0001):\n",
    "        optimizer = Adam(learning_rate = learning_rate)\n",
    "        self.model.compile(optimizer = optimizer, \n",
    "                           loss = self._calculate_combined_loss,\n",
    "                           )\n",
    "    \n",
    "    def train(self, x_train, batch_size, num_epochs):\n",
    "        self.model.fit(x_train, \n",
    "                      x_train,\n",
    "                      batch_size = batch_size,\n",
    "                      epochs = num_epochs,\n",
    "                      shuffle = True)\n",
    "        \n",
    "    def save(self, save_folder='.'):\n",
    "        self._create_folder_if_needed(save_folder)\n",
    "        self._save_parameters(save_folder)\n",
    "        self._save_weights(save_folder)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, save_folder='.'):\n",
    "        parameters_path = os.path.join(save_folder, 'parameters.pkl')\n",
    "        with open(parameters_path, 'rb') as f:\n",
    "            parameters = pickle.load(f)\n",
    "        # make autoencoder object\n",
    "        autoencoder = VAE(*parameters)\n",
    "        # load weights\n",
    "        weights_path = os.path.join(save_folder, 'weights.h5')\n",
    "        autoencoder.model.load_weights(weights_path)\n",
    "        return autoencoder\n",
    "\n",
    "    def reconstruct(self, images):\n",
    "        latent_representations = self.encoder.predict(images)\n",
    "        reconstructed_representations = self.decoder.predict(latent_representations)\n",
    "        return reconstructed_representations, latent_representations\n",
    "        \n",
    "    def _calculate_combined_loss(self,y_target, y_predicted):\n",
    "        reconstruction_loss = self._calculate_reconstruction_loss(y_target, y_predicted)\n",
    "        kl_loss = self._calculate_kl_loss(y_target, y_predicted)\n",
    "        combined_loss = self.alpha * reconstruction_loss + kl_loss\n",
    "        return combined_loss\n",
    "    \n",
    "    def _calculate_reconstruction_loss(self, y_target, y_predicted):\n",
    "        error = y_target - y_predicted\n",
    "        reconstruction_loss = K.mean(K.square(error), axis = [1, 2, 3])\n",
    "        return reconstruction_loss\n",
    "    \n",
    "    def _calculate_kl_loss(self, y_target, y_predicted):\n",
    "        kl_loss = - 0.5 * K.sum(1 + self.log_variance - K.square(self.mu) - \n",
    "                                K.exp(self.log_variance), axis = 1)\n",
    "        return kl_loss\n",
    "    \n",
    "    def _load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "    \n",
    "    def _create_folder_if_needed(self, folder_name):\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "            \n",
    "    def _save_parameters(self, folder_name):\n",
    "        parameters = [\n",
    "            self.input_shape, \n",
    "            self.conv_filters,\n",
    "            self.conv_kernels,\n",
    "            self.conv_strides,\n",
    "            self.latent_space_dim\n",
    "        ]\n",
    "        save_path = os.path.join(folder_name, 'parameters.pkl')\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(parameters, f)\n",
    "            \n",
    "    def _save_weights(self, folder_name):\n",
    "        save_path = os.path.join(folder_name, 'weights.h5')\n",
    "        self.model.save_weights(save_path)\n",
    "        \n",
    "    def _build(self):\n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "        self._build_autoencoder() \n",
    "        \n",
    "    def _build_encoder(self):\n",
    "        encoder_input = self._add_encoder_input()\n",
    "        self._model_input = encoder_input\n",
    "        conv_layers = self._add_conv_layers(encoder_input)\n",
    "        bottleneck = self._add_bottleneck(conv_layers)\n",
    "        self.encoder = Model(encoder_input, bottleneck, name = 'encoder')\n",
    "        \n",
    "    def _add_encoder_input(self):\n",
    "        return Input(shape = self.input_shape, name = 'encoder_input')\n",
    "    \n",
    "    def _add_conv_layers(self, encoder_input):\n",
    "        layer_graph = encoder_input\n",
    "        for i in range(self._num_conv_layers):\n",
    "            layer_graph = self._add_conv_layer(i, layer_graph)\n",
    "        return layer_graph\n",
    "    \n",
    "    def _add_conv_layer(self, layer_index, layer_graph):\n",
    "        # conv2D + ReLu + batch normalization\n",
    "        \n",
    "        current_layer = layer_index + 1\n",
    "        conv_layer = Conv2D(\n",
    "            filters = self.conv_filters[layer_index],\n",
    "            kernel_size = self.conv_kernels[layer_index],\n",
    "            strides = self.conv_strides[layer_index],\n",
    "            padding = 'same',\n",
    "            name = f'encoder_conv_layer_{current_layer}'\n",
    "        )\n",
    "        \n",
    "        layer_graph = conv_layer(layer_graph)\n",
    "        layer_graph = ReLU(name=f'encoder_relu_{current_layer}')(layer_graph)\n",
    "        layer_graph = BatchNormalization(name=f'encoder_bn_{current_layer}')(layer_graph)\n",
    "        \n",
    "        return layer_graph\n",
    "    \n",
    "    def _add_bottleneck(self, layer_graph): \n",
    "        # save shape for decoding\n",
    "        self._shape_before_bottleneck = K.int_shape(layer_graph)[1:]\n",
    "        \n",
    "        # flatten data and add bottleneck with Gaussian sampling\n",
    "        layer_graph = Flatten()(layer_graph)\n",
    "        \n",
    "        # create two branches of dense layers, one for the mean vector, one for log variance vector:\n",
    "        self.mu = Dense(self.latent_space_dim, name = 'mu')(layer_graph)\n",
    "        self.log_variance = Dense(self.latent_space_dim, \n",
    "                                  name = 'log_variance')(layer_graph)\n",
    "        \n",
    "        def sample_point_from_normal_distribution(args):\n",
    "            mu, log_variance = args\n",
    "            epsilon = K.random_normal(shape = K.shape(self.mu), \n",
    "                                      mean = 0., \n",
    "                                      stddev = 1.)\n",
    "            sampled_point = mu + K.exp(log_variance / 2) * epsilon\n",
    "            return sampled_point\n",
    "            \n",
    "        # merge two layers (wrapping function in graph using Lambda)\n",
    "        layer_graph = Lambda(sample_point_from_normal_distribution, \n",
    "                             name = 'encoder_output')([self.mu, self.log_variance])\n",
    "        \n",
    "        return layer_graph\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        decoder_input = self._add_decoder_input()\n",
    "        dense_layer = self._add_dense_layer(decoder_input)\n",
    "        reshape_layer = self._add_reshape_layer(dense_layer)\n",
    "        conv_transpose_layers = self._add_conv_transpose_layers(reshape_layer)\n",
    "        decoder_output = self._add_decoder_output(conv_transpose_layers)\n",
    "        self.decoder = Model(decoder_input, decoder_output, name = 'decoder')\n",
    "\n",
    "    def _add_decoder_input(self):\n",
    "        return Input(shape = self.latent_space_dim, name = 'decoder_input')\n",
    "    \n",
    "    def _add_dense_layer(self, decoder_input):\n",
    "        num_neurons = np.prod(self._shape_before_bottleneck)\n",
    "        return Dense(num_neurons, name = 'decoder_dense')(decoder_input)\n",
    "       \n",
    "    def _add_reshape_layer(self, dense_layer):\n",
    "        return Reshape(self._shape_before_bottleneck)(dense_layer)\n",
    "    \n",
    "    def _add_conv_transpose_layers(self, layer_graph):\n",
    "        for i in reversed(range(1, self._num_conv_layers)): # ignore input layer\n",
    "            layer_graph = self._add_conv_transpose_layer(i, layer_graph)\n",
    "        return layer_graph\n",
    "    \n",
    "    def _add_conv_transpose_layer(self, layer_index, layer_graph):\n",
    "        layer_num = self._num_conv_layers - layer_index\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters = self.conv_filters[layer_index],\n",
    "            kernel_size = self.conv_kernels[layer_index],\n",
    "            strides = self.conv_strides[layer_index],\n",
    "            padding = 'same',\n",
    "            name = f'decoder_conv_transpose_layer_{layer_num}'\n",
    "        )\n",
    "        \n",
    "        layer_graph = conv_transpose_layer(layer_graph)\n",
    "        layer_graph = ReLU(name = f'decoder_ReLU_{layer_num}')(layer_graph)\n",
    "        layer_graph = BatchNormalization(name = f'decoder_bn_{layer_num}')(layer_graph)\n",
    "        \n",
    "        return layer_graph\n",
    "    \n",
    "    def _add_decoder_output(self, layer_graph):\n",
    "        conv_transpose_layer = Conv2DTranspose(\n",
    "            filters = 1,\n",
    "            kernel_size = self.conv_kernels[0],\n",
    "            strides = self.conv_strides[0],\n",
    "            padding = 'same',\n",
    "            name = f'decoder_conv_transpose_layer_{self._num_conv_layers}'\n",
    "        ) \n",
    "        \n",
    "        layer_graph = conv_transpose_layer(layer_graph)\n",
    "        output_layer = Activation('sigmoid', name = 'output_sigmoid_layer')(layer_graph)\n",
    "        return output_layer\n",
    "        \n",
    "    def _build_autoencoder(self):\n",
    "        model_input = self._model_input\n",
    "        model_output = self.decoder(self.encoder(model_input))\n",
    "        self.model = Model(model_input, model_output, name = 'autoencoder')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3282d5",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54aa9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, learning_rate, batch_size, epochs):\n",
    "    autoencoder = VAE(\n",
    "    input_shape =  (256, 64, 1),\n",
    "    conv_filters = (512,256,64,32),\n",
    "    conv_kernels = (3,3,3,3,3),\n",
    "    conv_strides = (2,2,2,2, (2,1)),\n",
    "    latent_space_dim = 128\n",
    "    )\n",
    "    \n",
    "    autoencoder.summary()\n",
    "    autoencoder.compile(learning_rate)\n",
    "    autoencoder.train(x_train, batch_size, epochs)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b53717b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_epochs = 150\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4867fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 256, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " encoder_conv_layer_1 (Conv2D)  (None, 128, 32, 512  5120        ['encoder_input[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " encoder_relu_1 (ReLU)          (None, 128, 32, 512  0           ['encoder_conv_layer_1[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " encoder_bn_1 (BatchNormalizati  (None, 128, 32, 512  2048       ['encoder_relu_1[0][0]']         \n",
      " on)                            )                                                                 \n",
      "                                                                                                  \n",
      " encoder_conv_layer_2 (Conv2D)  (None, 64, 16, 256)  1179904     ['encoder_bn_1[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_relu_2 (ReLU)          (None, 64, 16, 256)  0           ['encoder_conv_layer_2[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_2 (BatchNormalizati  (None, 64, 16, 256)  1024       ['encoder_relu_2[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_conv_layer_3 (Conv2D)  (None, 32, 8, 64)    147520      ['encoder_bn_2[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_relu_3 (ReLU)          (None, 32, 8, 64)    0           ['encoder_conv_layer_3[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_3 (BatchNormalizati  (None, 32, 8, 64)   256         ['encoder_relu_3[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " encoder_conv_layer_4 (Conv2D)  (None, 16, 4, 32)    18464       ['encoder_bn_3[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_relu_4 (ReLU)          (None, 16, 4, 32)    0           ['encoder_conv_layer_4[0][0]']   \n",
      "                                                                                                  \n",
      " encoder_bn_4 (BatchNormalizati  (None, 16, 4, 32)   128         ['encoder_relu_4[0][0]']         \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2048)         0           ['encoder_bn_4[0][0]']           \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 128)          262272      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " log_variance (Dense)           (None, 128)          262272      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 128)          0           ['mu[0][0]',                     \n",
      "                                                                  'log_variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,879,008\n",
      "Trainable params: 1,877,280\n",
      "Non-trainable params: 1,728\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 128)]             0         \n",
      "                                                                 \n",
      " decoder_dense (Dense)       (None, 2048)              264192    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 16, 4, 32)         0         \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 32, 8, 32)        9248      \n",
      " r_1 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " decoder_ReLU_1 (ReLU)       (None, 32, 8, 32)         0         \n",
      "                                                                 \n",
      " decoder_bn_1 (BatchNormaliz  (None, 32, 8, 32)        128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 64, 16, 64)       18496     \n",
      " r_2 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " decoder_ReLU_2 (ReLU)       (None, 64, 16, 64)        0         \n",
      "                                                                 \n",
      " decoder_bn_2 (BatchNormaliz  (None, 64, 16, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 128, 32, 256)     147712    \n",
      " r_3 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " decoder_ReLU_3 (ReLU)       (None, 128, 32, 256)      0         \n",
      "                                                                 \n",
      " decoder_bn_3 (BatchNormaliz  (None, 128, 32, 256)     1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " decoder_conv_transpose_laye  (None, 256, 64, 1)       2305      \n",
      " r_4 (Conv2DTranspose)                                           \n",
      "                                                                 \n",
      " output_sigmoid_layer (Activ  (None, 256, 64, 1)       0         \n",
      " ation)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 443,361\n",
      "Trainable params: 442,657\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 256, 64, 1)]      0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 128)               1879008   \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 256, 64, 1)        443361    \n",
      "                                                                 \n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 2,322,369\n",
      "Trainable params: 2,319,937\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n",
      "Train on 500 samples\n",
      "Epoch 1/150\n",
      "500/500 [==============================] - 46s 92ms/sample - loss: 951770051.5840\n",
      "Epoch 2/150\n",
      "500/500 [==============================] - 44s 87ms/sample - loss: 943082822.1440\n",
      "Epoch 3/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 937035906.0480\n",
      "Epoch 4/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 933842627.5840\n",
      "Epoch 5/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 932202521.6000\n",
      "Epoch 6/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 931194878.9760\n",
      "Epoch 7/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 930442639.3600\n",
      "Epoch 8/150\n",
      "500/500 [==============================] - 41s 81ms/sample - loss: 929973445.1200\n",
      "Epoch 9/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 929530373.6320\n",
      "Epoch 10/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 929202714.6240\n",
      "Epoch 11/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 928857057.7920\n",
      "Epoch 12/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 928533202.4320\n",
      "Epoch 13/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 928255378.4320\n",
      "Epoch 14/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 928011886.0800\n",
      "Epoch 15/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 927784786.9440\n",
      "Epoch 16/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927624001.5360\n",
      "Epoch 17/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927501017.0880\n",
      "Epoch 18/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927408710.1440\n",
      "Epoch 19/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927348222.4640\n",
      "Epoch 20/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927304055.2960\n",
      "Epoch 21/150\n",
      "500/500 [==============================] - 43s 85ms/sample - loss: 927271451.6480\n",
      "Epoch 22/150\n",
      "500/500 [==============================] - 40s 81ms/sample - loss: 927245444.0960\n",
      "Epoch 23/150\n",
      "500/500 [==============================] - 42s 85ms/sample - loss: 927224482.8160\n",
      "Epoch 24/150\n",
      "500/500 [==============================] - 43s 86ms/sample - loss: 927206807.0400\n",
      "Epoch 25/150\n",
      "500/500 [==============================] - 43s 86ms/sample - loss: 927190758.9120\n",
      "Epoch 26/150\n",
      "500/500 [==============================] - 42s 84ms/sample - loss: 927177102.8480\n",
      "Epoch 27/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927165905.9200\n",
      "Epoch 28/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927156011.5200\n",
      "Epoch 29/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927147540.9920\n",
      "Epoch 30/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927139825.1520\n",
      "Epoch 31/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927133035.0080\n",
      "Epoch 32/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927126837.2480\n",
      "Epoch 33/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927121467.9040\n",
      "Epoch 34/150\n",
      "500/500 [==============================] - 40s 81ms/sample - loss: 927116214.7840\n",
      "Epoch 35/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927111813.1200\n",
      "Epoch 36/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927107597.8240\n",
      "Epoch 37/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927103459.8400\n",
      "Epoch 38/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927099959.2960\n",
      "Epoch 39/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927096717.3120\n",
      "Epoch 40/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927093577.7280\n",
      "Epoch 41/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927090728.4480\n",
      "Epoch 42/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927088133.6320\n",
      "Epoch 43/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927085526.5280\n",
      "Epoch 44/150\n",
      "500/500 [==============================] - 38s 77ms/sample - loss: 927083445.2480\n",
      "Epoch 45/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927081283.5840\n",
      "Epoch 46/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927079221.2480\n",
      "Epoch 47/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927077455.8720\n",
      "Epoch 48/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927075533.8240\n",
      "Epoch 49/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927073925.6320\n",
      "Epoch 50/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927072339.4560\n",
      "Epoch 51/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927070615.5520\n",
      "Epoch 52/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927069151.2320\n",
      "Epoch 53/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927067842.5600\n",
      "Epoch 54/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927066227.7120\n",
      "Epoch 55/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 927064863.7440\n",
      "Epoch 56/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927063564.2880\n",
      "Epoch 57/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 927061371.9040\n",
      "Epoch 58/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927056716.8000\n",
      "Epoch 59/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927052710.9120\n",
      "Epoch 60/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927050897.4080\n",
      "Epoch 61/150\n",
      "500/500 [==============================] - 38s 77ms/sample - loss: 927049530.3680\n",
      "Epoch 62/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927047291.9040\n",
      "Epoch 63/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 927041755.1360\n",
      "Epoch 64/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 927030925.3120\n",
      "Epoch 65/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927017313.2800\n",
      "Epoch 66/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 927004137.4720\n",
      "Epoch 67/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 926963156.4800\n",
      "Epoch 68/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 926884269.0560\n",
      "Epoch 69/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 926750884.3520\n",
      "Epoch 70/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 926606530.0480\n",
      "Epoch 71/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 926501409.7920\n",
      "Epoch 72/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 926440690.6880\n",
      "Epoch 73/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 926387175.4240\n",
      "Epoch 74/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 926328611.8400\n",
      "Epoch 75/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 926267036.6720\n",
      "Epoch 76/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 926204083.7120\n",
      "Epoch 77/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 926116562.9440\n",
      "Epoch 78/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 926043159.5520\n",
      "Epoch 79/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925993794.0480\n",
      "Epoch 80/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925961742.8480\n",
      "Epoch 81/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925941167.6160\n",
      "Epoch 82/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925925572.0960\n",
      "Epoch 83/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925913627.1360\n",
      "Epoch 84/150\n",
      "500/500 [==============================] - 38s 76ms/sample - loss: 925903968.7680\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 40s 79ms/sample - loss: 925898178.5600\n",
      "Epoch 86/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925891231.2320\n",
      "Epoch 87/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925886464.0000\n",
      "Epoch 88/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925879801.3440\n",
      "Epoch 89/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925875659.2640\n",
      "Epoch 90/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925871135.7440\n",
      "Epoch 91/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925868329.9840\n",
      "Epoch 92/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925865452.0320\n",
      "Epoch 93/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925861683.2000\n",
      "Epoch 94/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925858808.8320\n",
      "Epoch 95/150\n",
      "500/500 [==============================] - 40s 81ms/sample - loss: 925854369.7920\n",
      "Epoch 96/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925849170.9440\n",
      "Epoch 97/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925843153.4080\n",
      "Epoch 98/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925837100.0320\n",
      "Epoch 99/150\n",
      "500/500 [==============================] - 38s 76ms/sample - loss: 925829355.5200\n",
      "Epoch 100/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925824697.3440\n",
      "Epoch 101/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925820040.7040\n",
      "Epoch 102/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925816339.4560\n",
      "Epoch 103/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925821902.8480\n",
      "Epoch 104/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925833535.4880\n",
      "Epoch 105/150\n",
      "500/500 [==============================] - 38s 77ms/sample - loss: 925823265.2800\n",
      "Epoch 106/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925817687.0400\n",
      "Epoch 107/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 925813249.0240\n",
      "Epoch 108/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925809270.7840\n",
      "Epoch 109/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925807845.8880\n",
      "Epoch 110/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925805997.0560\n",
      "Epoch 111/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925802080.7680\n",
      "Epoch 112/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925799790.5920\n",
      "Epoch 113/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925797734.9120\n",
      "Epoch 114/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925791499.2640\n",
      "Epoch 115/150\n",
      "500/500 [==============================] - 38s 77ms/sample - loss: 925762406.9120\n",
      "Epoch 116/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925713024.0000\n",
      "Epoch 117/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925644613.6320\n",
      "Epoch 118/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925568525.8240\n",
      "Epoch 119/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925532438.5280\n",
      "Epoch 120/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925512366.5920\n",
      "Epoch 121/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925490844.6720\n",
      "Epoch 122/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925474823.6800\n",
      "Epoch 123/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925460675.5840\n",
      "Epoch 124/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925447752.7040\n",
      "Epoch 125/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925437378.0480\n",
      "Epoch 126/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925429574.6560\n",
      "Epoch 127/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925421896.1920\n",
      "Epoch 128/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925419333.1200\n",
      "Epoch 129/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 925414274.5600\n",
      "Epoch 130/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925410686.9760\n",
      "Epoch 131/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925407977.9840\n",
      "Epoch 132/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925404046.3360\n",
      "Epoch 133/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925401479.6800\n",
      "Epoch 134/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925398318.0800\n",
      "Epoch 135/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925397410.3040\n",
      "Epoch 136/150\n",
      "500/500 [==============================] - 40s 79ms/sample - loss: 925394886.6560\n",
      "Epoch 137/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925394839.5520\n",
      "Epoch 138/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925394334.7200\n",
      "Epoch 139/150\n",
      "500/500 [==============================] - 38s 77ms/sample - loss: 925391583.7440\n",
      "Epoch 140/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925391017.9840\n",
      "Epoch 141/150\n",
      "500/500 [==============================] - 38s 76ms/sample - loss: 925390134.7840\n",
      "Epoch 142/150\n",
      "500/500 [==============================] - 40s 80ms/sample - loss: 925387082.7520\n",
      "Epoch 143/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925386639.3600\n",
      "Epoch 144/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925385075.2000\n",
      "Epoch 145/150\n",
      "500/500 [==============================] - 38s 76ms/sample - loss: 925384594.9440\n",
      "Epoch 146/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925383153.6640\n",
      "Epoch 147/150\n",
      "500/500 [==============================] - 39s 78ms/sample - loss: 925381469.6960\n",
      "Epoch 148/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925380589.0560\n",
      "Epoch 149/150\n",
      "500/500 [==============================] - 39s 77ms/sample - loss: 925379785.7280\n",
      "Epoch 150/150\n",
      "500/500 [==============================] - 39s 79ms/sample - loss: 925379446.7840\n"
     ]
    }
   ],
   "source": [
    "autoencoder = train(inputs, learning_rate, batch_size, num_epochs)\n",
    "autoencoder.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d97c246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VAE at 0x1a097987d60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
